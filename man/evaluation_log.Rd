% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation_log.R
\name{evaluation_log}
\alias{evaluation_log}
\alias{evaluation_log.cv}
\alias{print.evaluation_log}
\alias{evaluation_log.fm_xgb}
\alias{evaluation_log.fm_glmnet}
\alias{evaluation_log.model}
\alias{evaluation_log.multimodel}
\alias{evaluation_log.xgb.Booster}
\title{Evaluation log}
\usage{
evaluation_log(x, ...)

\method{evaluation_log}{cv}(
  x,
  metric = x$metric[1],
  eval_weights = extract_model(x, 1)$weights,
  na.rm = FALSE,
  ...
)

\method{print}{evaluation_log}(
  x,
  se = getOption("cv_show_se"),
  n = getOption("print_max_model"),
  n_row = getOption("print_rows_evaluation_log"),
  digits = 3,
  ...
)

\method{evaluation_log}{fm_xgb}(
  x,
  label = deparse(substitute(x))[[1]],
  metric = NULL,
  eval_weights = weights(x),
  ...,
  .data = eval.parent(x$call$data)
)

\method{evaluation_log}{fm_glmnet}(
  x,
  label = deparse(substitute(x))[[1]],
  metric = NULL,
  eval_weights = weights(x),
  na.rm = FALSE,
  ...,
  .data = eval.parent(x$call$data)
)

\method{evaluation_log}{model}(x, metric = NULL, eval_weights = weights(x), ...)

\method{evaluation_log}{multimodel}(
  x,
  metric = NULL,
  eval_weights = extract_model(x, 1)$weights,
  ...
)

\method{evaluation_log}{xgb.Booster}(x, label = deparse(substitute(x))[[1]], ...)
}
\arguments{
\item{x}{Object of class \dQuote{cv}, \dQuote{model}, \dQuote{multimodel} or other.}

\item{\dots}{Arguments passed to methods.}

\item{metric}{A metric (see \code{\link{metrics}}), specified either as a character string (name of the metric function),
or as a named list of length 1, as in \code{list(rmse = rmse)}.
\code{metric=NULL} selects the default metric, see \code{\link{default_metric}}.}

\item{eval_weights}{Evaluation weights; see the \dQuote{Evaluation weights} in the
\dQuote{Details} section of \code{?modeltuner}.
\verb{"eval_weights=default} means \dQuote{use fitting weights} while \verb{"eval_weights=NULL}
means unweighted evaluation.}

\item{na.rm}{Logical: Whether NA values should be excluded from computations.}

\item{se}{Logical: Show standard errors?}

\item{n}{Integer: Information will be printed for \code{n} models at most.}

\item{n_row}{Integer: Evaluation log tables are printed by selecting \code{n_row} rows,
approximately equally distributed from first to last iteration.
Iteration corresponding to the choice according to criteria (\code{"min"} etc.) are added.}

\item{digits}{Number of digits to be printed.}

\item{label}{Model label}

\item{.data}{Passing data - for internal use.}
}
\value{
\code{evaluation_log()} returns an object of class \dQuote{evaluation_log}.
If \code{x} has no evaluation log, a (essentially empty) dummy object is returned.
}
\description{
This function is related to the concept of \emph{preferred iterations} in the context iteratively fitted models (IFM).
See \link{ifm} and \code{vignette("ifm")} for more information on the peculiarities of this type of models.
An evaluation log is essentially the collection of the training and test error for all iterations of an IFM
(or a related \dQuote{cv} object).

The most appealing way of displaying an evaluation log is usually by plotting it,
see \code{\link{plot.evaluation_log}}.
}
\details{
The evaluation log of a \dQuote{model} (or \emph{fitted model}) includes the training error only,
while the evaluation log of a \emph{cross-validated model} (object of class \dQuote{cv}) has both training and test errors.

If there are several models and no \code{metric} is specified, the \code{\link{default_metric}} from the first model is used.
If \code{x} includes models having different \code{weights}, \code{evaluation_log()} will use \code{eval_weights=NULL}.

Running \code{evaluation_log()} with a non-default \code{metric} requires a \dQuote{cv} object
that has been created with \code{keep_fits=TRUE}.

Whenever there are \code{NA} predictions and \code{na.rm=TRUE}, the errors are calculated based on observations
with non-NA predictions for \emph{all} iterations, such that the same subset of observations is used for all iterations.
}
\examples{
# Evaluation log of a 'fm_xgb' model
fitted_xgb <- fm_xgb(Sepal.Length ~ ., iris, max_depth = 2)
evaluation_log(fitted_xgb)    # evaluation log of a model has no 
plot(evaluation_log(fitted_xgb))

# Evaluation log of cross-validated 'fm_xgb' model
cv_xgb <- cv(model(fitted_xgb, label = "xgb_depth2"))
evaluation_log(cv_xgb) 
plot(evaluation_log(cv_xgb))

# Evaluation log of several cross-validated models
mydata <- simuldat()
fitted_glmnet <- fm_glmnet(Y ~ ., mydata)
cv_glmnet <- cv(multimodel(fitted_glmnet, prefix = "glmnet", alpha = 0:1))
label(cv_glmnet) <- c("ridge", "lasso")
evaluation_log(cv_glmnet)
plot(evaluation_log(cv_glmnet))

}
\seealso{
\code{\link{ifm}}, \code{\link{plot.evaluation_log}}, \code{\link{fm_xgb}}, \code{\link{fm_glmnet}}
}
