% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune.R
\name{tune}
\alias{tune}
\alias{tune.cv}
\alias{tune.multimodel}
\alias{tune.model}
\alias{tune.default}
\title{Selection of the best-performing model in a \dQuote{cv} object}
\usage{
tune(x, ...)

\method{tune}{cv}(x, metric = x$metric[1], label = NULL, ...)

\method{tune}{multimodel}(
  x,
  nfold = getOption("cv_nfold"),
  folds = NULL,
  metric = NULL,
  label = NULL,
  ...
)

\method{tune}{model}(
  x,
  ...,
  nfold = getOption("cv_nfold"),
  folds = NULL,
  metric = NULL,
  expand = TRUE,
  max_n_model = getOption("expand_max_model"),
  label = NULL,
  verbose = getOption("cv_verbose")
)

\method{tune}{default}(
  x,
  ...,
  nfold = getOption("cv_nfold"),
  folds = NULL,
  metric = NULL,
  expand = TRUE,
  max_n_model = getOption("expand_max_model"),
  verbose = getOption("cv_verbose"),
  use_original_args = FALSE,
  force = FALSE
)
}
\arguments{
\item{x}{An object of class \dQuote{\link{cv}}, or any other object that can be transformed to a \code{cv}.}

\item{\dots}{See \dQuote{Details} and \dQuote{Methods}.}

\item{metric}{A metric (see \code{\link{metrics}}), specified either as a character string (name of the metric function),
or as a named list of length 1, as in \code{list(rmse = rmse)}.
\code{metric=NULL} selects the default metric, see \code{\link{default_metric}}.}

\item{label}{A character string: The label of the output model. If \code{NULL}, the selected model's current label is kept.}

\item{nfold, folds}{Passed to \code{\link{make_folds}()}.}

\item{expand}{Logical: Expand the \dQuote{...} arguments (default) or join them element-wise?
If \code{expand=TRUE}, the vectors in \dQuote{...} will be expanded, the number of models will equal the product
of the lengths of the \dQuote{...} arguments;
otherwise, all \dQuote{...} arguments must have equal lengths, and the number of models will be equal to their common length.}

\item{max_n_model}{Maximal number of models that will be included.
If the number of specified models is greater than \code{max_n_model}, a subset will be selected at random.}

\item{verbose}{Passed to \code{\link{cv}()}.}

\item{use_original_args, force}{Arguments passed to \code{\link{fit}()}.}
}
\value{
All methods return an object of class \dQuote{model}, except \code{tune.default()},
which returns a fitted model of the same class as its input \code{x}.
}
\description{
\code{tune()} picks the \dQuote{best} model from a set of models, that is the model with
the smallest test error.
}
\details{
The core method is \code{tune.cv(x)}, that selects the model from those included in \code{x} having lowest test error
(in terms of the \code{metric}).
All other methods will run \code{tune.cv()} after some preprocessing steps -- see section \dQuote{Methods}.

For \emph{iteratively fitted models} (\link{ifm}, classes \dQuote{\link{fm_xgb}} and \dQuote{\link{fm_glmnet}}),
\code{tune(x)} without additional arguments returns the model corresponding to the \emph{preferred iteration}, thus
tuning the parameter \code{nrounds} or \code{lambda}, respectively.
For other models, \code{tune(x)} simply returns \code{x}.

Note the different role of the \dQuote{...} arguments in different methods:
\itemize{
\item{In \code{tune.cv()} and \code{tune.multimodel()} they are passed to \code{\link{cv_performance}()},
allowing specification of \code{eval_weights};}
\item{In \code{tune.model()} and \code{tune.default()}, they are passed to \code{\link{multimodel}()}.
This allows expanding a selection of alternative parameterizations, of which that with the smallest test error
is finally returned.
Selecting the best-performing parameterization thus reduces to one simple line of code: \cr
\code{tune(mymodel, hyperparms=candidate_values)}}
}
}
\section{Methods}{

\itemize{
\item{\code{tune.cv}: Executes \code{cv_performance(x, ...)}, finds the model with minimal test error and extracts that model
(using \code{\link{extract_model}}).}
\item{\code{tune.multimodel(x, ...)} essentially runs \code{x \%>\% cv \%>\% tune(...)}.}
\item{\code{tune.model(x, ...)} essentially runs \code{x \%>\% multimodel(...) \%>\% cv \%>\% tune}.}
\item{\code{tune.default(x, ...)} is applied to a fitted model \code{x} and essentially runs
\code{x \%>\% model \%>\% multimodel(...) \%>\% cv \%>\% tune \%>\% fit}.}
}
}

\examples{
options(cv_nfold = 1/3)  # accelerates cross-validations, see ?modeltuner_options

# Tune xgboost parameter:
model_xgb <- model("fm_xgb", Sepal.Length~., iris, class = "fm_xgb")
mm_xgb <- multimodel(model_xgb, max_depth = 1:5)
cv_xgb <- cv(mm_xgb)
plot(cv_performance(cv_xgb), 
     xvar = "max_depth", zeroline = FALSE)

# tune() automatically selects the best-performing model:
tuned1 <- tune(mm_xgb, max_depth = 1:5, label = "tuned_model_1")
tuned1
\donttest{
# Several tuning steps in a pipe:
tuned2 <- tuned1 |> 
  tune(learning_rate = c(0.1, 0.3, 1)) |> 
  tune(min_child_weight = c(5, 10, 20), label = "tuned_model_2")
fit(tuned2, eval = FALSE, use_original_args = TRUE)  # extract selected model
}
# Alternatively test a number of random parameterizations
tuned3 <- tune(tuned1, learning_rate = c(0.1, 0.3, 1), 
               min_child_weight = c(5, 10, 20), 
               label = "tuned_model_3")
fit(tuned3, eval = FALSE, use_original_args = TRUE)  # extract selected model

}
\seealso{
\code{\link{multimodel}}, \code{\link{cv}}, \code{\link{cv_performance}}, \code{\link{fit}}
}
