% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv_performance.R
\name{cv_performance}
\alias{cv_performance}
\alias{cv_performance.cv}
\alias{cv_performance.model}
\alias{cv_performance.multimodel}
\alias{cv_performance.default}
\alias{print.performance}
\alias{plot.performance}
\title{Calculate train and test errors based on cross-validation.}
\usage{
cv_performance(x, ...)

\method{cv_performance}{cv}(
  x,
  metric = x$metric[1],
  eval_weights = "default",
  na.rm = FALSE,
  param = TRUE,
  ...
)

\method{cv_performance}{model}(x, metric = NULL, eval_weights = "default", na.rm = FALSE, ...)

\method{cv_performance}{multimodel}(x, metric = NULL, eval_weights = "default", na.rm = FALSE, ...)

\method{cv_performance}{default}(x, metric = NULL, eval_weights = "default", na.rm = FALSE, ...)

\method{print}{performance}(
  x,
  se = getOption("cv_show_se"),
  n = getOption("print_max_row"),
  digits = 5,
  param = TRUE,
  ...
)

\method{plot}{performance}(
  x,
  xvar = "model",
  errorbars = getOption("cv_show_se"),
  plot = TRUE,
  size = 2,
  lwd = 1,
  lwd_errorbars = 0.5,
  zeroline = TRUE,
  alpha = 0.3,
  ...
)
}
\arguments{
\item{x}{\dQuote{\link{cv}} object, or object of another class.}

\item{\dots}{Passed to the \code{metric} function.}

\item{metric}{A metric (see \code{\link{metrics}}), specified either as a character string (name of the metric function),
or as a named list of length 1, as in \code{list(rmse = rmse)}.
\code{metric=NULL} selects the default metric, see \code{\link{default_metric}}.}

\item{eval_weights}{Evaluation weights; see the \dQuote{Evaluation weights} in the
\dQuote{Details} section of \code{?modeltuner}.
\verb{"eval_weights=default} means \dQuote{use fitting weights} while \verb{"eval_weights=NULL}
means unweighted evaluation.}

\item{na.rm}{Logical: Whether NA values should be excluded from computations.}

\item{param}{Logical: Keep parameters from the parameter table in the output?}

\item{se}{Logical: Show standard errors?}

\item{n}{Integer: Maximal number of rows to print.}

\item{digits}{Integer: Number of digits to print.}

\item{xvar}{If \code{xvar} if not specified (default), a bar plot is drawn.
Alternatively, a line plot is generated, with \code{xvar} as the variable on x axis.
\code{xvar} should be a character string, the name of a numeric variable in the performance table \code{x}.
Typically, the \code{xvar} is some hyperparameter varying across models.}

\item{errorbars}{Logical: Whether to add error bars in plots.}

\item{plot}{Logical: If \code{TRUE}, a ggplot is returned, if \code{FALSE} a \code{data.frame}.
\code{plot()} first prepares a \code{data.frame} and then draws some ggplot using this data,
with limited options for customization.
If you want to design your own plot, you can set \code{plot=FALSE}, and use the \code{data.frame} returned by \code{plot()}
to create your plot.}

\item{size}{Graphic detail: Size of point.}

\item{lwd}{Graphic detail: Line width of interpolating line.}

\item{lwd_errorbars}{Graphic detail: Line width of errorbars.}

\item{zeroline}{Logical: Whether to include a horizontal reference line at level 0.}

\item{alpha}{Graphic detail: Opacity of bars.}
}
\value{
\code{cv_performance()} returns a performance table.
This is a \link{param_table} with additional class \dQuote{performance} having some additional information stored in its attributes.
Each row corresponds to a model. It has columns \code{train_}\emph{metric} and \code{test_}\emph{metric}
(e.g., \code{train_rmse} and \code{test_rmse}), \code{se_train_}\emph{metric} and \code{se_test_}\emph{metric},
\code{time_cv} (execution time of the cross-validation), and possibly
more columns being part of the parameter table of the multimodel (see \dQuote{Details} section in \code{\link{multimodel}}).
}
\description{
\code{cv_performance} returns a \emph{performance table}, a summary table of training and test errors for the models included in the
main argument \code{x}, of class \dQuote{performance}.
\code{cv_performance.cv()} is the core method. All other methods run this method after some preprocessing steps â€“
see section \dQuote{Methods}.

The method \code{plot.performance()} generates a graphical display of the performance values in the \dQuote{performance} object \code{x},
a bar chart by default, alternatively a line plot (depending on parameter \code{xvar}).
}
\details{
While different models in a \dQuote{cv} object can have different \code{metric}s, \code{cv_performance()} always reports the same metric
for all models.
If \code{metric} is not specified in the call to \code{cv_performance()}, the metric from the \emph{first model} will be chosen
(see \code{\link{default_metric}}).
If \code{cv_performance()} is applied to a \dQuote{cv} object including models having different default \code{weights}
(and \code{weights} are not given explicitly), \code{cv_performance()} will use \code{eval_weights=NULL}.

\strong{Details on evaluation:} \cr
For each \code{fold}, the evaluation metric is calculated separately for the sets of training and test observations,
yielding \eqn{k} pairs \eqn{(train\_err_i, test\_err_i)}, \eqn{i=1, \ldots, k}, where \eqn{k} is the number of folds.
\code{cv_performance()} reports the average of the \eqn{train\_err_i}, \eqn{i=1, \ldots, k}, as the training error and
the average of the \eqn{test\_err_i} as the test error.
In case of non-NULL \code{eval_weights}, weighted averages are calculated with group weights computed as
the group-wise sums of the observations weights.

Standard errors of the reported errors are only printed if you set \code{se=TRUE} when printing the performance table,
which is not the case by default (see the option \code{cv_show_se}, cf. \code{\link{modeltuner_options}}).
These standard errors are only reported if the number of folds is \verb{>1}.
Their computation is based on the assumption of perfect independence of all residuals, and may thus be very unreliable.
As a rough guide, the standard errors are reasonable in case of a model with just a few free parameters and many observations,
while they will severely underestimate the actual uncertainty in the case of models of high structural complexity.
}
\section{Methods}{

\itemize{
\item{\code{cv_performance.cv()} is the core method described above.
It uses the first cross-validated model's metric as default metric.}
\item{\code{cv_performance.model(x, ...)} executes \code{x \%>\% cv \%>\% cv_performance(...)}.}
\item{\code{cv_performance.multimodel(x, ...)} executes \code{x \%>\% cv \%>\% cv_performance(...)}.
Its (implicit) default metric is \code{\link{default_metric}(x)}.}
\item{\code{cv_performance.default(x, ...)} executes \code{x \%>\% model \%>\% cv \%>\% cv_performance(...)}, where \code{x} is a fitted model.}
}
}

\examples{
# iris data: compare several model approaches
mm <- c(
  lm = model(lm(Sepal.Length ~., iris)), 
  lm2 = model(lm(Sepal.Length ~.^2, iris)), 
  glmnet = model(fm_glmnet(Sepal.Length ~., iris)), 
  glmnet2 = model(fm_glmnet(Sepal.Length ~.^2, iris)), 
  fm_xgb = model(fm_xgb(Sepal.Length ~., iris)))
cvobj <- cv(mm, nfold = 5)

# performance
cvperm <- cv_performance(cvobj)
cvperm

# Sort by test error
sort_models(cvperm, by = "test")

#' print performance table with estimated standard errors (unreliable!)
print(cvperm, se = TRUE)

}
\seealso{
The sections on \dQuote{Metrics} and  \dQuote{Evaluation weights}
in \code{?\link{modeltuner}}; \code{\link{metrics}}, \code{\link{subset}}, \code{\link{sort_models}}.
}
