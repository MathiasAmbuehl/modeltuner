% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fm_xgb.R
\name{fm_xgb}
\alias{fm_xgb}
\alias{print.fm_xgb}
\alias{predict.fm_xgb}
\alias{extract_booster}
\title{\code{formula}-based wrapper for \code{xgb.train()}}
\usage{
fm_xgb(
  formula,
  data,
  nrounds = 100,
  early_stopping_rounds = 10,
  weights = NULL,
  na.action = na.pass,
  verbose = interactive(),
  monotone_constraints = 0,
  interaction_constraints = NULL,
  obj = NULL,
  feval = NULL,
  maximize = FALSE,
  pref_iter = NULL,
  keep_x = TRUE,
  ...
)

\method{print}{fm_xgb}(x, abbreviate = TRUE, ...)

\method{predict}{fm_xgb}(object, newdata, pref_iter = object$pref_iter, ...)

extract_booster(object)
}
\arguments{
\item{formula}{A \code{formula}.}

\item{data}{A \code{data.frame}.}

\item{nrounds, early_stopping_rounds, obj, feval, maximize}{Passed to \code{xgb.train}
(but note that some default values are different).}

\item{weights}{Fitting weights.}

\item{na.action}{A function which indicates what should happen when the data contain \code{NA}s.
\code{\link{na.pass}} is the default, \code{\link{na.omit}}, \code{\link{na.exclude}} or \code{\link{na.fail}}
could be meaningful alternative settings.}

\item{verbose}{Logical: Whether to print information on progress to console.}

\item{monotone_constraints}{Named vector with values in \code{c(-1, 0, 1)}.
Names identify features, \code{1} means increasing, \code{-1} decreasing and \code{0} no constraint.
Features not appearing will be assigned a \code{0} in the call to \code{xgb.train()}.
Default is no constraints.}

\item{interaction_constraints}{List of named character vectors defining interaction constraints. Default is no constraints.}

\item{pref_iter}{An integer, the \emph{preferred iteration}. This is the iteration that is used by default
when predictions from the model are computed with \code{predict()}.
If \code{pref_iter=NULL}, the \emph{last} iteration will be used.
See \code{\link{ifm}} and \code{vignette("ifm")} for information on the concepts of
\emph{iteratively fitted models} and preferred iterations.
The preferred iteration of a model can be changed without re-fitting the model,
see \code{\link{set_pref_iter}()}.}

\item{keep_x}{Logical: Whether to keep the model matrix \code{x} as a component of the return value.}

\item{...}{Passed to \code{params} in \code{xgb.train()}.}

\item{x, object}{Object of class \dQuote{fm_xgb}.}

\item{abbreviate}{Logical. If \code{TRUE} (the default), long formulas and calls are printed in abbreviated mode,
such that they usually fit on 4 or fewer output lines; otherwise they are printed entirely, no matter how long they are.}

\item{newdata}{Data for prediction.}
}
\value{
\code{fm_xgb()} returns a list of class \dQuote{fm_xgb} with components
\itemize{
\item{\emph{booster}: the xgboost booster, of class \dQuote{xgb.Booster};}
\item{\emph{formula}: the formula;}
\item{\emph{x}: the model matrix (resulting from the \code{formula} using \code{model.matrix()});}
\item{\emph{weights}: the fitting weights;}
\item{\emph{xlevels}: list of the levels of the factors included in the model;}
\item{\emph{pref_iter}: the preferred iteration, an integer (see argument \code{pref_iter});}
\item{\emph{na.action}: the \code{\link{na.action}} used during data preparation;}
\item{\emph{contrasts}: the \code{\link{contrasts}} used during data preparation;}
\item{\emph{call}: the matched call generating the model.}
}

\code{extract_booster()} returns the booster, of class \dQuote{xgb.Booster}.
}
\description{
\code{fm_xgb()} is a convenience wrapper for tree boosting with
\code{\link[xgboost]{xgb.train}()} (from package \CRANpkg{xgboost})
that fits into the \code{modeltuner} framework.
The model is specified by the arguments \code{formula} and \code{data}.
The resulting models belong to the class of so-called \emph{iteratively fitted models},
see \code{\link{ifm}} and \code{vignette("ifm")} for information.
}
\details{
Not all parameters of \code{xgb.train()} are available in \code{fm_xgb()}.
In particular, those related to console output (\code{verbose}, \code{print_every_n}),
those related to saving the result (\code{save_period}, \code{save_name}) and \code{callbacks}
won't be passed to \code{xgb.train()}.
The parameters \code{x} and \code{y} to be passed to \code{xgb.train()} are extracted from
\code{formula} and \code{data} by means of \code{\link{model.frame}}, \code{\link{model.matrix}}
and \code{\link{model.response}}.

\emph{Features of cross-validation of models generated with} \code{fm_xgb()}:
\itemize{
\item{The model class \dQuote{fm_xgb} belongs to the class of so-called
\emph{iteratively fitted models}; see \link{ifm} and \code{vifnette("ifm")} for information on
the peculiarities of cross-validating such models. In particular,
note the role of the parameter \code{iter} in \code{\link{cv}()}.}
\item{When \code{cv()} is executed with \code{keep_fits=TRUE}, the fitted models from
cross-validation that are stored in the result (and returned by \code{\link{extract_fits}()})
will not be of class \dQuote{fm_xgb}, but of class \dQuote{xgb.Booster},}
}

\emph{Default metric:}
Currently, xgboost models generated with \code{fm_xgb()} are the only models not having the default choice
of its metric, \code{rmse} for continuous response and \code{logLoss} in the binary case.
Each xgboost model has an \code{eval_metric}.
If not specified explicitly by the user, this metric is automatically chosen depending on the \code{objective}
in the call to \code{xgb.train()} or \code{fm_xgb()}.
In \code{modeltuner}, when \code{cv()} is applied, the \code{eval_metric} is taken as the default metric
of the resulting "cv" object.
(see \code{\link{default_metric}()}).

\code{extract_booster()} returns the \emph{booster}, an object of class \dQuote{xgb.Booster},
as returned by \code{\link[xgboost]{xgb.train}()}.
}
\section{Methods (by generic)}{
\itemize{
\item \code{print(fm_xgb)}: \code{print()} method

\item \code{predict(fm_xgb)}: \code{predict()} method

}}
\examples{
# mtcars data
xgb_cars <- fm_xgb(mpg ~ ., mtcars)
# predict
predict(xgb_cars, newdata = head(mtcars))

# iris data
xgb_iris <- fm_xgb(Sepal.Width ~ ., iris)
# cross-validate
cv(xgb_iris)
# Plot evaluation log
plot(evaluation_log(last_cv()))

}
\seealso{
\code{\link[xgboost]{xgb.train}}, \code{\link{xgb.cv}} (package \CRANpkg{xgboost});
\link{ifm} and \code{vignette("ifm")}; \code{\link{default_metric}};
\code{\link{fit.model_fm_xgb}}; \code{\link{set_pref_iter}}
}
