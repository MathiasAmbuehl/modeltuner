<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="modeltuner">
<title>Fitting and tuning Iteratively Fitted Models • modeltuner</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Fitting and tuning Iteratively Fitted Models">
<meta property="og:description" content="modeltuner">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">modeltuner</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.9</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/modeltuner.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/ifm.html">Fitting and tuning Iteratively Fitted Models</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Fitting and tuning Iteratively Fitted Models</h1>
                        <h4 data-toc-skip class="author">Mathias
Ambuehl</h4>
            
            <h4 data-toc-skip class="date">2023-09-22</h4>
      
      
      <div class="d-none name"><code>ifm.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="iteratively-fitted-models">Iteratively Fitted Models<a class="anchor" aria-label="anchor" href="#iteratively-fitted-models"></a>
</h2>
<p><em>Introductory remark:</em> In this vignette, it is assumed that
the reader is familiar with the main concepts of the
<strong>modeltuner</strong> package, in particular the classes “model”,
“multimodel”, “cv” and the related functions and methods. If this is not
the case, reading the introductory vignette “An introduction to
modeltuner with examples” (<code>vignette(modeltuner)</code>) first is
recommended.</p>
<p>We speak of an <strong>iteratively fitted model (IFM)</strong> when
the output of the model fitting function is not just a single model, but
rather <em>a sequence of models of increasing structural
complexity</em>. Examples are gradient boosting (as implemented in
package <strong>xgboost</strong>) or Lasso regression and elastic nets
(available from package <strong>glmnet</strong>).</p>
<p>Such a sequence of models begins with a simple model (often a
constant model) and ends with a model that possibly seriously overfits
the training data. Within the sequence, training error typically
decreases successively, while test error decreases in early iterations,
then at some point stagnates or starts increasing again.</p>
<p>We use the term <strong>iterations</strong> to refer to element
models in the sequence, although properly speaking, the actual fitting
process does not need be iterative. In the two cases mentioned above,
gradient boosting and regularized linear models, the fitting process
actually is iterative. In the analysis tools to be introduced in this
vignette, the <em>availability of predictions for each iteration</em> is
crucial, such that training and test error can be computed for each
iteration.</p>
<div class="section level3">
<h3 id="the-bias-variance-trade-off">The bias-variance trade-off<a class="anchor" aria-label="anchor" href="#the-bias-variance-trade-off"></a>
</h3>
<p>When working with IFMs, one of the the key tasks consists in finding
an appropriate iteration or model, one that is not too simple and at the
same time does not excessively (over)fit the training data. In short, we
strive for a good compromise in a situation of bias-variance trade-off.
While a simple model suffers from bias, a model that approximates the
training data too closely has the disadvantage of a large variance.</p>
</div>
</div>
<div class="section level2">
<h2 id="boosted-trees-fm_xgb">Boosted trees: <code>fm_xgb()</code><a class="anchor" aria-label="anchor" href="#boosted-trees-fm_xgb"></a>
</h2>
<div class="section level3">
<h3 id="a-single-model">A single model<a class="anchor" aria-label="anchor" href="#a-single-model"></a>
</h3>
<p>This example uses simulated data generated by the function
<code><a href="../reference/simuldat.html">simuldat()</a></code>, included in <code>modeltuner</code>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://mathiasambuehl.github.io/modeltuner/">modeltuner</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>cv_verbose <span class="op">=</span> <span class="cn">FALSE</span>, width <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> </span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org" class="external-link">magrittr</a></span>, warn.conflicts <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>   <span class="co"># for pipe operator</span></span>
<span></span>
<span><span class="co"># Simulated data set</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/simuldat.html">simuldat</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>We can define an xgboost model in terms of <code>formula</code> and
<code>data</code> with the function <code><a href="../reference/fm_xgb.html">fm_xgb()</a></code>:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fm0</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fm_xgb.html">fm_xgb</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">d</span><span class="op">)</span></span>
<span><span class="va">fm0</span></span></code></pre></div>
<pre><code><span><span class="co">## Fitted model of class 'fm_xgb' </span></span>
<span><span class="co">##   formula:     Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g - 1</span></span>
<span><span class="co">##   data:        d (500 rows)</span></span>
<span><span class="co">##   call:        fm_xgb(formula = Y ~ ., data = d)</span></span>
<span><span class="co">##   nfeatures:   30</span></span>
<span><span class="co">##   iterations:  100</span></span>
<span><span class="co">##   pref_iter:   100</span></span></code></pre>
<p><code><a href="../reference/model.html">model()</a></code> creates an object of class “model”:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Model</span></span>
<span><span class="va">m0</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model.html">model</a></span><span class="op">(</span><span class="va">fm0</span>, label <span class="op">=</span> <span class="st">"xgb"</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="evaluation-log-of-a-model">Evaluation log of a model<a class="anchor" aria-label="anchor" href="#evaluation-log-of-a-model"></a>
</h4>
<p>A model’s <code><a href="../reference/evaluation_log.html">evaluation_log()</a></code> consists in the sequence of
training errors, but does not provide any indication as to where a good
balance between oversimplifying and overfitting is to be found.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># evaluation log of an object of class "model"</span></span>
<span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">m0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 'evaluation_log', 1 model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model 'xgb':</span></span>
<span><span class="co">##   model class: fm_xgb</span></span>
<span><span class="co">##  iter train_rmse test_rmse</span></span>
<span><span class="co">##     1     3.5772        NA</span></span>
<span><span class="co">##    21     0.3939        NA</span></span>
<span><span class="co">##    41     0.1484        NA</span></span>
<span><span class="co">##    60     0.0671        NA</span></span>
<span><span class="co">##    80     0.0268        NA</span></span>
<span><span class="co">##   100     0.0123        NA</span></span></code></pre>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">m0</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Warning: Removed 100 rows containing missing values (`geom_point()`).</span></span></code></pre>
<pre><code><span><span class="co">## Warning: Removed 100 rows containing missing values (`geom_line()`).</span></span></code></pre>
<p><img src="ifm_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>Note that we could also have applied <code><a href="../reference/evaluation_log.html">evaluation_log()</a></code> to
the <em>fitted</em> model <code>fm0</code>.</p>
</div>
<div class="section level4">
<h4 id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h4>
<p>Running a cross validation with <code><a href="../reference/cv.html">cv()</a></code> yields a
cross-validated model, of class “cv”. The evaluation log now includes
both training and test error.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># evaluation log of an object of class "cv"</span></span>
<span><span class="va">cvm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m0</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 'evaluation_log', 1 cross-validated model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model 'xgb':</span></span>
<span><span class="co">##   model class: fm_xgb</span></span>
<span><span class="co">##  iter train_rmse test_rmse criterion</span></span>
<span><span class="co">##     1     3.5736      3.77          </span></span>
<span><span class="co">##    15     0.5281      2.11          </span></span>
<span><span class="co">##    30     0.2326      2.05          </span></span>
<span><span class="co">##    44     0.1239      2.03          </span></span>
<span><span class="co">##    59     0.0650      2.03          </span></span>
<span><span class="co">##    63     0.0542      2.03       min</span></span>
<span><span class="co">##    73     0.0361      2.03</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-5-1.png" width="672"></p>
<p>In this plot, the vertical line marks the iteration having minimal
test error. By default, this iteration is considered the
<em>preferred</em> one. In the example, iteration 63 is preferred.
<code><a href="../reference/cv_performance.html">cv_performance()</a></code> returns training and test error recorded
at this iteration:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##     train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## xgb   0.054174    2.0271        63   0.951</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="back-to-model-and-fitted-model">Back to model and fitted model<a class="anchor" aria-label="anchor" href="#back-to-model-and-fitted-model"></a>
</h4>
<p>If we now take a step back, extracting the <code>model</code> object
from <code>cvm</code>, the information on the best (preferred) iteration
is attached to the resulting <code>model</code> object:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># back to model</span></span>
<span><span class="va">m1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/extract_model.html">extract_model</a></span><span class="op">(</span><span class="va">cvm</span><span class="op">)</span> <span class="co"># or: tune(cvm)</span></span>
<span><span class="va">m1</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "model" object ---</span></span>
<span><span class="co">##   label:          xgb</span></span>
<span><span class="co">##   model class:    fm_xgb</span></span>
<span><span class="co">##   formula:        Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g - 1</span></span>
<span><span class="co">##   data:           data.frame [500 x 12], input as: 'data = d'</span></span>
<span><span class="co">##   response_type:  continuous</span></span>
<span><span class="co">##   call:           fm_xgb(formula = Y ~ ., data = data)</span></span>
<span><span class="co">##   fit:            Object of class 'fm_xgb'</span></span>
<span><span class="co">## Preferred iteration from cv:  iter=63</span></span></code></pre>
<p>In the final line of the output above, the best iteration according
to cross-validation results is reported. Apart from that,
<code>m1</code> is identical with the original model object
<code>m0</code>.</p>
<p>Finally, on fitting this model, thus converting a “model” object to a
fitted model, the <code>nrounds</code> argument is adjusted according to
the preference resulting from cross-validation:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># back to fitted model</span></span>
<span><span class="va">fm1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fit.html">fit</a></span><span class="op">(</span><span class="va">m1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## set_pref_iter(), model 'xgb', modifications made in call:</span></span>
<span><span class="co">##   pref_iter=63, nrounds=63, early_stopping_rounds=NULL</span></span></code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">getCall</a></span><span class="op">(</span><span class="va">fm1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## fm_xgb(formula = Y ~ ., data = d, nrounds = 63L, pref_iter = 63L, </span></span>
<span><span class="co">##     early_stopping_rounds = NULL)</span></span></code></pre>
<p>Note that the number of rounds has been fixed to
<code>nrounds=</code>63 in the fitting call.</p>
</div>
<div class="section level4">
<h4 id="tuning-nrounds-with-tune">Tuning <code>nrounds</code> with <code>tune()</code><a class="anchor" aria-label="anchor" href="#tuning-nrounds-with-tune"></a>
</h4>
<p>Applying <code><a href="../reference/tune.html">tune()</a></code> to a IFM without further arguments
triggers a cross-validation and returns the adjusted model
(<code>model</code> or fitted) with “tuned” number of rounds:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/tune.html">tune</a></span><span class="op">(</span><span class="va">m0</span><span class="op">)</span>  <span class="co"># executes m0 %&gt;% cv %&gt;% extract_model</span></span>
<span><span class="fu"><a href="../reference/tune.html">tune</a></span><span class="op">(</span><span class="va">fm0</span><span class="op">)</span> <span class="co"># executes fm0 %&gt;% model %&gt;% cv %&gt;% extract_model %&gt;% fit</span></span></code></pre></div>
<p>Repeated executions will result in varying choices of
<code>nrounds</code>, as new cross-validation groups (folds) are
randomly generated at each execution, leading to different a evaluation
log.</p>
</div>
</div>
<div class="section level3">
<h3 id="default-metric-for-xgboost-models">Default metric for xgboost models<a class="anchor" aria-label="anchor" href="#default-metric-for-xgboost-models"></a>
</h3>
<p>“cv” objects based on a model created with <code><a href="../reference/fm_xgb.html">fm_xgb()</a></code>
differ from any other model class in the way how their metric is
selected by default. They currently are the only model type not having
the standard choice, which is <code>rmse</code> for continuous response
and <code>logLoss</code> in the binary case.</p>
<p>Each xgboost model has an <code>eval_metric</code>. If not specified
explicitly by the user, this metric is set depending on the argument
<code>objective</code> in the call to <code>xgb.train()</code> or
<code><a href="../reference/fm_xgb.html">fm_xgb()</a></code>. For example, <code>objective="reg:linear"</code>
has <code>rmse</code> as its default <code>eval_metric</code>, while
<code>objective="binary:logistic"</code> chooses <code>logloss</code>.
In <strong>modeltuner</strong>, the <code>eval_metric</code> is taken as
the default metric of the resulting “cv” object.</p>
<p>As an example, consider a model using the <code>eval_metric</code>
“mphe” (mean pseudo-Huber error). This is the default
<code>eval_metric</code> for
<code>objective="reg:pseudohubererror"</code>.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">diamonds</span>, package <span class="op">=</span> <span class="st">"ggplot2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">diamonds</span> <span class="op">&lt;-</span> <span class="va">diamonds</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">diamonds</span><span class="op">)</span>, <span class="fl">5000</span><span class="op">)</span>, <span class="op">]</span></span>
<span><span class="va">xgb_phe</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model.html">model</a></span><span class="op">(</span><span class="fu"><a href="../reference/fm_xgb.html">fm_xgb</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">price</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span> , <span class="va">diamonds</span>, objective <span class="op">=</span> <span class="st">"reg:pseudohubererror"</span><span class="op">)</span>,</span>
<span>                 base_score <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html" class="external-link">median</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">diamonds</span><span class="op">$</span><span class="va">price</span><span class="op">)</span><span class="op">)</span>,</span>
<span>            label <span class="op">=</span> <span class="st">"xgb_pseudohuber"</span><span class="op">)</span></span>
<span><span class="va">cv_phe</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">xgb_phe</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cv_phe</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: mphe</span></span>
<span><span class="co">##                 train_mphe test_mphe iteration time_cv</span></span>
<span><span class="co">## xgb_pseudohuber  0.0010635 0.0062366        99   5.215</span></span></code></pre>
<p>The values of the metric “mphe” (and the evaluation log) are read
from the return value of <code>xgb.train()</code> or
<code>xgb.cv()</code> - we actually don’t necessarily need an R function
<code>mphe()</code> evaluating this metric in the
<code>modeltuner</code> implementation.</p>
<p><code><a href="../reference/cv_performance.html">cv_performance()</a></code> can still be executed with any
available metric:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cv_phe</span>, metric <span class="op">=</span> <span class="st">"mae"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: mae</span></span>
<span><span class="co">##                 train_mae test_mae iteration time_cv</span></span>
<span><span class="co">## xgb_pseudohuber  0.033856  0.08174        99   5.215</span></span></code></pre>
<p>In case of doubt on the default metric of a model, one may use the
function <code><a href="../reference/default_metric.html">default_metric()</a></code>. It identifies the metric
function that will be used by default:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/default_metric.html">default_metric</a></span><span class="op">(</span><span class="va">xgb_phe</span><span class="op">)</span>  <span class="co"># no metric function, only a name (values read from xgboost output)</span></span></code></pre></div>
<pre><code><span><span class="co">## $mphe</span></span>
<span><span class="co">## NULL</span></span></code></pre>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/default_metric.html">default_metric</a></span><span class="op">(</span><span class="va">m1</span><span class="op">)</span>       <span class="co"># metric available, only its name</span></span></code></pre></div>
<pre><code><span><span class="co">## $rmse</span></span>
<span><span class="co">## function (actual, predicted, w = NULL, ...) </span></span>
<span><span class="co">## {</span></span>
<span><span class="co">##     sqrt(mse(actual = actual, predicted = predicted, w = w, ...))</span></span>
<span><span class="co">## }</span></span>
<span><span class="co">## &lt;bytecode: 0x555572646ad8&gt;</span></span>
<span><span class="co">## &lt;environment: namespace:MetricsWeighted&gt;</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="hyperparameter-tuning">Hyperparameter tuning<a class="anchor" aria-label="anchor" href="#hyperparameter-tuning"></a>
</h3>
<p>In this section, we investigate the effect of modifying the value of
parameter <code>max_depth</code> in model <code>m0</code>. A multimodel
is defined by expanding <code>max_depth</code> from 1 to 6:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate a multimodel</span></span>
<span><span class="va">mm_depth</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/multimodel.html">multimodel</a></span><span class="op">(</span><span class="va">m0</span>, prefix <span class="op">=</span> <span class="st">"xgb_depth"</span>, max_depth <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, </span>
<span>                       nrounds <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="cross-validation-1">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation-1"></a>
</h4>
<p><code><a href="../reference/cv.html">cv()</a></code> runs separate cross-validations for the 6 models,
using identical folds for all of them.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvmm_depth</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">mm_depth</span><span class="op">)</span></span>
<span><span class="va">cvmm_depth</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 6 validated models ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  500</span></span>
<span><span class="co">##   Number of test sets:     10</span></span>
<span><span class="co">##   Size of test sets:       50</span></span>
<span><span class="co">##   Size of training sets:  450</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Models:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth1':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth2':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth3':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## and 3 models more, labelled:</span></span>
<span><span class="co">##   'xgb_depth4', 'xgb_depth5', 'xgb_depth6'</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Parameter table:</span></span>
<span><span class="co">##            max_depth nrounds</span></span>
<span><span class="co">## xgb_depth1         1     200</span></span>
<span><span class="co">## xgb_depth2         2     200</span></span>
<span><span class="co">## xgb_depth3         3     200</span></span>
<span><span class="co">## ... 3 rows omitted (nrow=6)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth1':  min (iter=144)</span></span>
<span><span class="co">##   model 'xgb_depth2':  min (iter=151)</span></span>
<span><span class="co">##   model 'xgb_depth3':  min (iter=91)</span></span>
<span><span class="co">## ... and 3 lines more.</span></span></code></pre>
<p>The printed output lists details for the first 3 models only by
default. In order to get display details on all 6 models output, we
could use the argument <code>n</code> in <code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code> (output
not shown):</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">cvmm_depth</span>, n <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></span></code></pre></div>
<p>The function <code><a href="../reference/set_pref_iter.html">extract_pref_iter()</a></code> extracts the preferred
iterations from a “cv” object:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">extract_pref_iter</a></span><span class="op">(</span><span class="va">cvmm_depth</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth1':  min (iter=144)</span></span>
<span><span class="co">##   model 'xgb_depth2':  min (iter=151)</span></span>
<span><span class="co">##   model 'xgb_depth3':  min (iter=91)</span></span>
<span><span class="co">##   model 'xgb_depth4':  min (iter=117)</span></span>
<span><span class="co">##   model 'xgb_depth5':  min (iter=99)</span></span>
<span><span class="co">##   model 'xgb_depth6':  min (iter=67)</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cv_performance-and-evaluation_log">
<code>cv_performance()</code> and <code>evaluation_log()</code><a class="anchor" aria-label="anchor" href="#cv_performance-and-evaluation_log"></a>
</h4>
<p>The output of these functions (be it <code>print</code>ed or
<code>plot</code>ted) extends what we have seen for a single model above
to the multiple model case:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvmm_depth</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-17-1.png" width="672"></p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvmm_depth</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-17-2.png" width="672"></p>
<p>The plot of the performance table can be enhanced by setting
<code>xvar=max_depth</code>:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvmm_depth</span><span class="op">)</span>, xvar <span class="op">=</span> <span class="st">"max_depth"</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-18-1.png" width="672"></p>
<p>The plot shows how values <code>max_depth&gt;2</code> do not improve
the predictive performance on unseen data, but increasingly overfit the
training data.</p>
</div>
<div class="section level4">
<h4 id="tuning-and-extracting-a-model-or-fitted-model-from-a-cv-object">Tuning and extracting a model or fitted model from a “cv”
object<a class="anchor" aria-label="anchor" href="#tuning-and-extracting-a-model-or-fitted-model-from-a-cv-object"></a>
</h4>
<p>Applying <code><a href="../reference/tune.html">tune()</a></code> to <code>cvmm_depth</code> will pick the
best-performing model (that having lowest test error) and return it.</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m_tuned</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tune.html">tune</a></span><span class="op">(</span><span class="va">cvmm_depth</span><span class="op">)</span></span>
<span><span class="va">fm_tuned</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fit.html">fit</a></span><span class="op">(</span><span class="va">m_tuned</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## set_pref_iter(), model 'xgb_depth2', modifications made in call:</span></span>
<span><span class="co">##   pref_iter=151, nrounds=151, early_stopping_rounds=NULL</span></span></code></pre>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">getCall</a></span><span class="op">(</span><span class="va">fm_tuned</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## fm_xgb(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + </span></span>
<span><span class="co">##     X9 + X10 + g, data = data, nrounds = 151L, early_stopping_rounds = NULL, </span></span>
<span><span class="co">##     pref_iter = 151L, max_depth = 2L)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="preference-criteria-for-iterations">Preference criteria for iterations<a class="anchor" aria-label="anchor" href="#preference-criteria-for-iterations"></a>
</h3>
<div class="section level4">
<h4 id="basics-on-preference-criteria">Basics on preference criteria<a class="anchor" aria-label="anchor" href="#basics-on-preference-criteria"></a>
</h4>
<p>So far, it has been assumed throughout that the preferred model is
the one having minimal test error. Depending on the current application
and its goals, other choices may be desirable. For example, some users
may want <em>stable</em> models in the sense that they expect that
fitting the model with different data sets yields results that differ no
too much. In the example of multimodel <code>mm</code> above, the model
with <code>max_depth=2</code> has the lowest test error, but still the
analyst may favor choosing <code>max_depth=1</code>, as it exhibits less
overfitting.</p>
<p><code>modeltuner</code> offers various alternative
preference/selection criteria. The default criterion, used in all
examples so far, was</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/crit_iter.html">crit_min</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preference criterion for an iteratively fitted model:</span></span>
<span><span class="co">##   criterion:     crit_min()</span></span>
<span><span class="co">##   label suffix:  "min"</span></span>
<span><span class="co">##   Selects the iteration with minimal test error.</span></span></code></pre>
<p>To choose a criterion, you use the argument <code>iter</code> in
<code><a href="../reference/cv.html">cv()</a></code>. The call <code>cv(mm)</code> could also be written
as</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m0</span>, iter <span class="op">=</span> <span class="fu"><a href="../reference/crit_iter.html">crit_min</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="co"># same as: cvm &lt;- cv(m0)</span></span></code></pre></div>
<div class="section level5">
<h5 id="alternative-criteria">Alternative criteria<a class="anchor" aria-label="anchor" href="#alternative-criteria"></a>
</h5>
<p>Two criteria that avoid selecting a too heavily overfitting model are
<code><a href="../reference/crit_iter.html">crit_se()</a></code> and <code><a href="../reference/crit_iter.html">crit_overfit()</a></code>. They are
explained in the printed output below:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/crit_iter.html">crit_se</a></span><span class="op">(</span>factor <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>         <span class="co"># default: factor=1</span></span></code></pre></div>
<pre><code><span><span class="co">## Preference criterion for an iteratively fitted model:</span></span>
<span><span class="co">##   criterion:     crit_se(2)</span></span>
<span><span class="co">##   label suffix:  "2se"</span></span>
<span><span class="co">##   Selects the first iteration where test error does not exceed</span></span>
<span><span class="co">##     the minimal test error by more than 2 standard errors.</span></span></code></pre>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span>ratio <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span>  <span class="co"># default: ratio=0.9</span></span></code></pre></div>
<pre><code><span><span class="co">## Preference criterion for an iteratively fitted model:</span></span>
<span><span class="co">##   criterion:     crit_overfit(0.8)</span></span>
<span><span class="co">##   label suffix:  "overfit0.8"</span></span>
<span><span class="co">##   Selects the iteration with minimal test error among those where</span></span>
<span><span class="co">##   the ratio of training and test error does not fall below 0.8.</span></span></code></pre>
<p>More available criteria are</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/crit_iter.html">crit_first</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/crit_iter.html">crit_last</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/crit_iter.html">crit_iter</a></span><span class="op">(</span><span class="fl">20</span><span class="op">)</span> <span class="co"># fixed iteration number</span></span></code></pre></div>
</div>
</div>
<div class="section level4">
<h4 id="specifying-a-criterion-in-cv">Specifying a criterion in <code>cv()</code><a class="anchor" aria-label="anchor" href="#specifying-a-criterion-in-cv"></a>
</h4>
<p>We now cross-validate the model <code>m0</code> again, thereby
claiming that the ratio of training and test error remains above 0.75.
We do this by choosing the criterion
<code>iter = crit_overfit(0.75)</code>, thus ensuring that predictive
performances are compared between models sharing a similar degree of
overfitting.</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># cv()</span></span>
<span><span class="va">cvm_ovf075</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m0</span>, iter <span class="op">=</span> <span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span><span class="fl">0.75</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">cvm_ovf075</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 1 validated model ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  500</span></span>
<span><span class="co">##   Number of test sets:     10</span></span>
<span><span class="co">##   Size of test sets:       50</span></span>
<span><span class="co">##   Size of training sets:  450</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g - 1</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb':  overfit0.75 (iter=3)</span></span></code></pre>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm_ovf075</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-24-1.png" width="672"></p>
<p>As a consequence of the different choice of a preference criterion,
an earlier iteration (iteration 3) is selected than in <code>cvm1</code>
above with the default <code>iter = crit_min()</code>, where iteration
63 was preferred.</p>
</div>
<div class="section level4">
<h4 id="back-to-model-and-fitted-model-1">Back to model and fitted model<a class="anchor" aria-label="anchor" href="#back-to-model-and-fitted-model-1"></a>
</h4>
<p>The preferred iteration is transformed back to a <code>model</code>
and fitted model exactly as shown before in the <code><a href="../reference/crit_iter.html">crit_min()</a></code>
case:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m_tuned_ovf075</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/extract_model.html">extract_model</a></span><span class="op">(</span><span class="va">cvm_ovf075</span><span class="op">)</span></span>
<span><span class="va">fm_tuned_ovf075</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fit.html">fit</a></span><span class="op">(</span><span class="va">m_tuned_ovf075</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## set_pref_iter(), model 'xgb', modifications made in call:</span></span>
<span><span class="co">##   pref_iter=3, nrounds=3, early_stopping_rounds=NULL</span></span></code></pre>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">getCall</a></span><span class="op">(</span><span class="va">fm_tuned_ovf075</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## fm_xgb(formula = Y ~ ., data = d, nrounds = 3L, pref_iter = 3L, </span></span>
<span><span class="co">##     early_stopping_rounds = NULL)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="multimodel-case">Multimodel case<a class="anchor" aria-label="anchor" href="#multimodel-case"></a>
</h3>
<p>It is now straightforward to repeat the tuning of xgboost’s
hyperparameter <code>max_step</code> with preference
<code>iter = crit_overfit(0.75)</code>.</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvmm_ovf075</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">mm_depth</span>, iter <span class="op">=</span> <span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span><span class="fl">0.75</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/set_pref_iter.html">extract_pref_iter</a></span><span class="op">(</span><span class="va">cvmm_ovf075</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth1':  overfit0.75 (iter=112)</span></span>
<span><span class="co">##   model 'xgb_depth2':  overfit0.75 (iter=20)</span></span>
<span><span class="co">##   model 'xgb_depth3':  overfit0.75 (iter=9)</span></span>
<span><span class="co">##   model 'xgb_depth4':  overfit0.75 (iter=5)</span></span>
<span><span class="co">##   model 'xgb_depth5':  overfit0.75 (iter=3)</span></span>
<span><span class="co">##   model 'xgb_depth6':  overfit0.75 (iter=3)</span></span></code></pre>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvmm_ovf075</span><span class="op">)</span>, xvar <span class="op">=</span> <span class="st">"max_depth"</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-26-1.png" width="672"></p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvmm_ovf075</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html" class="external-link">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"bottom"</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-26-2.png" width="672"></p>
<p>In contrast to hyperparameter tuning based on
<code><a href="../reference/crit_iter.html">crit_min()</a></code>, where <code>max_depth=2</code> turned out to be
the choice, <code>tune(cvmm_ovf075)</code> picks
<code>max_depth=1</code>, which reflects the stronger reluctance to
complex and more overfitting models expressed in
<code>iter=crit_overfit(0.75)</code>:</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/tune.html">tune</a></span><span class="op">(</span><span class="va">cvmm_ovf075</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "model" object ---</span></span>
<span><span class="co">##   label:          xgb_depth1</span></span>
<span><span class="co">##   model class:    fm_xgb</span></span>
<span><span class="co">##   formula:        Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   data:           data.frame [500 x 12], input as: 'data = d'</span></span>
<span><span class="co">##   response_type:  continuous</span></span>
<span><span class="co">##   call:           fm_xgb(formula = Y ~ ., data = data, nrounds = 200, max_depth = 1L)</span></span>
<span><span class="co">## Preferred iteration from cv:  iter=112</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="multiple-preference-criteria">Multiple preference criteria<a class="anchor" aria-label="anchor" href="#multiple-preference-criteria"></a>
</h3>
<p><code><a href="../reference/cv.html">cv()</a></code> accepts multiple preference criteria. You just
enclose the list of criteria in <code>crit_list(...)</code> or in
<code>c(...)</code>. The example below uses a model with maximal tree
depth 2, and three preference criteria are given.</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m_depth2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">m0</span>, nround <span class="op">=</span> <span class="fl">200</span>, max_depth <span class="op">=</span> <span class="fl">2</span>, label <span class="op">=</span> <span class="st">"xgb_depth2"</span><span class="op">)</span></span>
<span><span class="va">cvm_depth2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m_depth2</span>, iter <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="../reference/crit_iter.html">crit_min</a></span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span><span class="fl">.7</span><span class="op">)</span>, <span class="fu"><a href="../reference/crit_iter.html">crit_last</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">cvm_depth2</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 1 validated model ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  500</span></span>
<span><span class="co">##   Number of test sets:     10</span></span>
<span><span class="co">##   Size of test sets:       50</span></span>
<span><span class="co">##   Size of training sets:  450</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth2':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth2':  min (iter=100), overfit0.7 (iter=28), last (iter=110)</span></span></code></pre>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm_depth2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-28-1.png" width="672"></p>
<p>The cross-validated model <code>cvm_depth2</code> includes three
preference criteria. Let’s call the first of them the <em>primary
criterion</em>. In the picture of the <code>evaluation_log</code> above,
the iteration corresponding to the primary criterion is shown with a
solid line, while all other criteria have a dotted line. In the printed
output of an evaluation log, the primary criterion is marked with an
asterisk:</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm_depth2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 'evaluation_log', 1 cross-validated model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model 'xgb_depth2':</span></span>
<span><span class="co">##   model class: fm_xgb</span></span>
<span><span class="co">##  iter train_rmse test_rmse  criterion</span></span>
<span><span class="co">##     1      3.805      3.85           </span></span>
<span><span class="co">##    23      1.548      2.09           </span></span>
<span><span class="co">##    28      1.449      2.06 overfit0.7</span></span>
<span><span class="co">##    45      1.231      1.95           </span></span>
<span><span class="co">##    66      1.063      1.89           </span></span>
<span><span class="co">##    88      0.937      1.88           </span></span>
<span><span class="co">##   100      0.878      1.86 min*      </span></span>
<span><span class="co">##   110      0.834      1.86 last</span></span></code></pre>
<p>The role of the primary criterion is that
<code><a href="../reference/cv_performance.html">cv_performance()</a></code> returns training and test error from that
iteration. The same is true for related functions, such as
<code><a href="../reference/cv_predict.html">cv_predict()</a></code> or <code><a href="../reference/plot.model.html">plot.cv()</a></code>.</p>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># cv_performance</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##            train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## xgb_depth2    0.87816    1.8584       100   0.564</span></span></code></pre>
<div class="section level4">
<h4 id="changing-the-primary-criterion-set_pref_iter">Changing the primary criterion: <code>set_pref_iter()</code><a class="anchor" aria-label="anchor" href="#changing-the-primary-criterion-set_pref_iter"></a>
</h4>
<p>The primary preference criterion of a “cv” object can be changed with
the function <code><a href="../reference/set_pref_iter.html">set_pref_iter()</a></code>.</p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvm_depth2_modif1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/set_pref_iter.html">set_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2</span>, iter <span class="op">=</span> <span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span><span class="fl">0.7</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm_depth2_modif1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-31-1.png" width="672"></p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">extract_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2_modif1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth2':  overfit0.7 (iter=28), min (iter=100), last (iter=110)</span></span></code></pre>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2_modif1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##            train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## xgb_depth2     1.4493    2.0601        28   0.564</span></span></code></pre>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># cv_performance() with different metric:</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2_modif1</span>, metric <span class="op">=</span> <span class="st">"medae"</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: medae</span></span>
<span><span class="co">##            train_medae test_medae iteration time_cv</span></span>
<span><span class="co">## xgb_depth2     0.95277     1.3459        28   0.564</span></span></code></pre>
<p><code><a href="../reference/set_pref_iter.html">set_pref_iter()</a></code> is most useful when you specify a
criterion that has previously been included in the cross-validation. If
a criterion is chosen, some results will not be available:</p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># set_pref_iter with criteria not stated before in cv(..., iter = )</span></span>
<span><span class="va">cvm_depth2_modif2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/set_pref_iter.html">set_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2</span>, <span class="fu"><a href="../reference/crit_iter.html">crit_iter</a></span><span class="op">(</span><span class="fl">50</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2_modif2</span><span class="op">)</span>  <span class="co"># available</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##            train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## xgb_depth2      1.183    1.9289        50   0.564</span></span></code></pre>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># cv_performance() with different metric:</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2_modif2</span>, metric <span class="op">=</span> <span class="st">"medae"</span><span class="op">)</span> <span class="co"># required predictions are not available</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: medae</span></span>
<span><span class="co">##            train_medae test_medae iteration time_cv</span></span>
<span><span class="co">## xgb_depth2          NA         NA        50   0.564</span></span></code></pre>
<p>Note the difference: <code><a href="../reference/cv_performance.html">cv_performance()</a></code> with the
non-default <code>metric="medae"</code> is available for
<code>cvm_depth2_modif1</code> but not for
<code>cvm_depth2_modif2</code>. The reason is that cross-validation
predictions for iterations selected in the original “cv” object
<code>cvm_depth2</code> (i.e. for iterations 100, 28, 110) are stored in
that object, but this is not the case for iteration 50. In order to
obtain predictions for iteration 50, we need to re-run a cross
validation, including <code>crit_iter(50)</code> in the list of criteria
(or we could also set <code>keep_fits=TRUE</code> in <code><a href="../reference/cv.html">cv()</a></code> -
see the section “Evaluation log for alternative metric” below).</p>
</div>
<div class="section level4">
<h4 id="expand_pref_iter">
<code>expand_pref_iter()</code><a class="anchor" aria-label="anchor" href="#expand_pref_iter"></a>
</h4>
<p>Next, we introduce the function <code><a href="../reference/set_pref_iter.html">expand_pref_iter()</a></code>,
which converts a “cv” object with a single cross-validated model and
multiple criteria to a “cv” object having several (essentially
identical) models, but different preference criteria.</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvm_depth2_modif3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/set_pref_iter.html">expand_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2</span><span class="op">)</span></span>
<span><span class="va">cvm_depth2_modif3</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 3 validated models ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  500</span></span>
<span><span class="co">##   Number of test sets:     10</span></span>
<span><span class="co">##   Size of test sets:       50</span></span>
<span><span class="co">##   Size of training sets:  450</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Models:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth2_min':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth2_overfit0.7':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_depth2_last':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + g</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth2_min':         min (iter=100)</span></span>
<span><span class="co">##   model 'xgb_depth2_overfit0.7':  overfit0.7 (iter=28)</span></span>
<span><span class="co">##   model 'xgb_depth2_last':        last (iter=110)</span></span></code></pre>
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cvm_depth2_modif3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##                       train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## xgb_depth2_min           0.87816    1.8584       100   0.564</span></span>
<span><span class="co">## xgb_depth2_overfit0.7    1.44932    2.0601        28   0.564</span></span>
<span><span class="co">## xgb_depth2_last          0.83375    1.8593       110   0.564</span></span></code></pre>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cvm_depth2_modif3</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-34-1.png" width="672"></p>
<p>The different structures of <code>cvm_depth2</code> and
<code>cvm_depth2_modif3</code> also becomes obvious when comparing the
respective output from <code><a href="../reference/set_pref_iter.html">extract_pref_iter()</a></code>:</p>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">extract_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth2':  min (iter=100), overfit0.7 (iter=28), last (iter=110)</span></span></code></pre>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">extract_pref_iter</a></span><span class="op">(</span><span class="va">cvm_depth2_modif3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_depth2_min':         min (iter=100)</span></span>
<span><span class="co">##   model 'xgb_depth2_overfit0.7':  overfit0.7 (iter=28)</span></span>
<span><span class="co">##   model 'xgb_depth2_last':        last (iter=110)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="evaluation-log-for-an-alternative-metric-and-set_metric">Evaluation log for an alternative metric and
<code>set_metric()</code><a class="anchor" aria-label="anchor" href="#evaluation-log-for-an-alternative-metric-and-set_metric"></a>
</h3>
<p>For the object <code>cv_phe</code> generated above (in the section on
“Default metric for xgboost models”), the <em>evaluation log</em>
summarizes the training and test errors for all iterations using the
default metric, as usual. Here, the default metric is <code>mphe</code>,
mean pseudo-Huber error:</p>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cv_phe</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="va">plot</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-36-1.png" width="672"></p>
<p>The performance expressed in an alternative metric (for the primary
preferred iteration) is obtained with</p>
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cv_phe</span>, metric <span class="op">=</span> <span class="st">"mae"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: mae</span></span>
<span><span class="co">##                 train_mae test_mae iteration time_cv</span></span>
<span><span class="co">## xgb_pseudohuber  0.033856  0.08174        99   5.215</span></span></code></pre>
<p>The computation of the performance table with a different metric
requires the predictions only from the preferred iterations. In
contrast, calculation of an evaluation log for a non-default metric
requires the predictions from all iterations. As a consequence, the
following call won’t produce a useful output (its result is full of
<code>NA</code>s):</p>
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cv_phe</span>, metric <span class="op">=</span> <span class="st">"mae"</span><span class="op">)</span></span></code></pre></div>
<p>If the computation of the evaluation log for an alternative metric is
required, one can execute the cross-validation setting
<code>keep_fits = TRUE</code> in <code><a href="../reference/cv.html">cv()</a></code>. This saves the fits
obtained during cross-validation as a part of the result, such that
calculation of predictions from all iterations is possible, thus
avoiding having to re-run the cross-validation.</p>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_phe_fits</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">xgb_phe</span>, keep_fits <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                  folds <span class="op">=</span> <span class="va">cv_phe</span><span class="op">$</span><span class="va">folds</span><span class="op">)</span> <span class="co"># use same folds as in cv_phe</span></span>
<span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cv_phe_fits</span>, metric <span class="op">=</span> <span class="st">"mae"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="va">plot</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-39-1.png" width="672"></p>
<div class="section level5">
<h5 id="set_metric">
<code>set_metric()</code><a class="anchor" aria-label="anchor" href="#set_metric"></a>
</h5>
<p>The function <code><a href="../reference/set_metric.html">set_metric()</a></code> changes the default metric of
a “cv” object, thereby computing the evaluation log of the model with
that metric. It also re-evaluates the preference criteria such that the
preferred iterations may be different from those in the original “cv”
object. <code><a href="../reference/set_metric.html">set_metric()</a></code> does not re-run a cross-validation,
but it requires that the fits are attached to the input “cv”.</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_phe_mae</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/set_metric.html">set_metric</a></span><span class="op">(</span><span class="va">cv_phe_fits</span>, <span class="st">"mae"</span><span class="op">)</span></span>
<span><span class="va">cv_phe_mae</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 1 validated model ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  5000</span></span>
<span><span class="co">##   Number of test sets:      10</span></span>
<span><span class="co">##   Size of test sets:       500</span></span>
<span><span class="co">##   Size of training sets:  4500</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'xgb_pseudohuber':</span></span>
<span><span class="co">##   model class:  fm_xgb</span></span>
<span><span class="co">##   formula:      log(price) ~ carat + cut + color + clarity + depth + table + x + y + </span></span>
<span><span class="co">##                     z - 1</span></span>
<span><span class="co">##   metric:       mae</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'xgb_pseudohuber':  min (iter=100)</span></span></code></pre>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cv_phe_mae</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: mae</span></span>
<span><span class="co">##                 train_mae test_mae iteration time_cv</span></span>
<span><span class="co">## xgb_pseudohuber  0.033588 0.081736       100   5.217</span></span></code></pre>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cv_phe_mae</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 'evaluation_log', 1 cross-validated model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model 'xgb_pseudohuber':</span></span>
<span><span class="co">##   model class: fm_xgb</span></span>
<span><span class="co">##  iter train_mae test_mae criterion</span></span>
<span><span class="co">##     1    0.3180   0.3220          </span></span>
<span><span class="co">##    21    0.0681   0.0918          </span></span>
<span><span class="co">##    41    0.0536   0.0850          </span></span>
<span><span class="co">##    60    0.0454   0.0831          </span></span>
<span><span class="co">##    80    0.0388   0.0821          </span></span>
<span><span class="co">##   100    0.0336   0.0817       min</span></span></code></pre>
</div>
</div>
</div>
<div class="section level2">
<h2 id="elastic-net-fm_glmnet">Elastic net: <code>fm_glmnet()</code><a class="anchor" aria-label="anchor" href="#elastic-net-fm_glmnet"></a>
</h2>
<p>In this shorter section on the second class on iteratively fitted
models, we use simulated data again. The code below generates a
model</p>
<p><span class="math display">\[Y = \beta_1 \cdot X_1 + \beta_2\cdot X_2
+ \ldots + \beta_{30}\cdot X_{30} + residual\]</span></p>
<p>where all <span class="math inline">\(X_i\)</span> and the residuals
are standard normal and <span class="math inline">\(\beta_i=1/(4i)\)</span> for all <span class="math inline">\(i\)</span>. The beta coefficient are decreasing
with <span class="math inline">\(i\)</span>, and we expect that many of
them will not have a significant effect on the response <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">500</span>; <span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">*</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span></span>
<span><span class="va">beta</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="op">(</span><span class="fl">4</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">x</span><span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span><span class="va">beta</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>, <span class="va">x</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html" class="external-link">rm</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<p>We fit a Lasso model, and run a cross-validation including criteria
<code><a href="../reference/crit_iter.html">crit_min()</a></code> and <code><a href="../reference/crit_iter.html">crit_se()</a></code>. The usage of the
features related to preference criteria and the selection of iterations
is exactly as in the <code>xgboost</code> case.</p>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fitted_glmnet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fm_glmnet.html">fm_glmnet</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span><span class="va">.</span>, <span class="va">d</span><span class="op">)</span>  <span class="co"># glmnet() with default alpha=1: LASSO</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fitted_glmnet</span><span class="op">)</span>  <span class="co"># similar to plot method for class 'glmnet', but a ggplot</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-42-1.png" width="672"></p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model_glmnet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/model.html">model</a></span><span class="op">(</span><span class="va">fitted_glmnet</span>, label <span class="op">=</span> <span class="st">"glmnet"</span><span class="op">)</span></span></code></pre></div>
<p>The application of <code><a href="../reference/cv.html">cv()</a></code>, <code><a href="../reference/cv_performance.html">cv_performance()</a></code>
and <code><a href="../reference/evaluation_log.html">evaluation_log()</a></code> follows the same logic as for xgboost
model:</p>
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv_glmnet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">model_glmnet</span>, iter <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="../reference/crit_iter.html">crit_min</a></span><span class="op">(</span><span class="op">)</span>, <span class="fu"><a href="../reference/crit_iter.html">crit_overfit</a></span><span class="op">(</span><span class="fl">.95</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">cv_glmnet</span></span></code></pre></div>
<pre><code><span><span class="co">## --- A "cv" object containing 1 validated model ---</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Validation procedure: Complete k-fold Cross-Validation</span></span>
<span><span class="co">##   Number of obs in data:  500</span></span>
<span><span class="co">##   Number of test sets:     10</span></span>
<span><span class="co">##   Size of test sets:       50</span></span>
<span><span class="co">##   Size of training sets:  450</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Model:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 'glmnet':</span></span>
<span><span class="co">##   model class:  fm_glmnet</span></span>
<span><span class="co">##   formula:      y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + </span></span>
<span><span class="co">##                     X14 + X15 + X16 + X17 + X18 + X19 + X20 + X21 + X22 + X23 + X24 + </span></span>
<span><span class="co">##                     X25 + X26 + X27 + X28 + X29 + X30 - 1</span></span>
<span><span class="co">##   metric:       rmse</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Preferred iterations:</span></span>
<span><span class="co">##   model 'glmnet':  min (iter=19), overfit0.95 (iter=19)</span></span></code></pre>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># performance table of cv_glmnet:</span></span>
<span><span class="fu"><a href="../reference/cv_performance.html">cv_performance</a></span><span class="op">(</span><span class="va">cv_glmnet</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##        train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## glmnet    0.97368   0.99584        19   0.237</span></span></code></pre>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot the evaluation log:</span></span>
<span><span class="fu"><a href="../reference/evaluation_log.html">evaluation_log</a></span><span class="op">(</span><span class="va">cv_glmnet</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span>zeroline <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="ifm_files/figure-html/unnamed-chunk-43-1.png" width="672"></p>
<div class="section level4">
<h4 id="set_pref_iter-and-expand_pref_iter">
<code>set_pref_iter()</code> and
<code>expand_pref_iter()</code><a class="anchor" aria-label="anchor" href="#set_pref_iter-and-expand_pref_iter"></a>
</h4>
<p>Again, the usage of these functions is similar to what has been shown
in the section on xgboost models.</p>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">set_pref_iter</a></span><span class="op">(</span><span class="va">cv_glmnet</span>, <span class="fu"><a href="../reference/crit_iter.html">crit_se</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="va">cv_performance</span> <span class="co"># different iteration</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##        train_rmse test_rmse   lambda iteration time_cv</span></span>
<span><span class="co">## glmnet    0.97368   0.99584 0.042681        19   0.237</span></span></code></pre>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/set_pref_iter.html">expand_pref_iter</a></span><span class="op">(</span><span class="va">cv_glmnet</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="va">cv_performance</span></span></code></pre></div>
<pre><code><span><span class="co">## --- Performance table ---</span></span>
<span><span class="co">## Metric: rmse</span></span>
<span><span class="co">##                    train_rmse test_rmse iteration time_cv</span></span>
<span><span class="co">## glmnet_min            0.97368   0.99584        19   0.237</span></span>
<span><span class="co">## glmnet_overfit0.95    0.97368   0.99584        19   0.237</span></span></code></pre>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Mathias Ambuehl.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
